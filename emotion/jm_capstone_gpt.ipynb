{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3140b4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a38a7252",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dark_knight\\anaconda3\\envs\\emotion\\lib\\site-packages\\matplotlib\\__init__.py:200: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(module.__version__) < minver:\n",
      "C:\\Users\\dark_knight\\anaconda3\\envs\\emotion\\lib\\site-packages\\setuptools\\_distutils\\version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "C:\\Users\\dark_knight\\anaconda3\\envs\\emotion\\lib\\site-packages\\matplotlib\\__init__.py:200: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(module.__version__) < minver:\n",
      "C:\\Users\\dark_knight\\anaconda3\\envs\\emotion\\lib\\site-packages\\setuptools\\_distutils\\version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "C:\\Users\\dark_knight\\anaconda3\\envs\\emotion\\lib\\site-packages\\matplotlib\\__init__.py:200: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(module.__version__) < minver:\n",
      "C:\\Users\\dark_knight\\anaconda3\\envs\\emotion\\lib\\site-packages\\setuptools\\_distutils\\version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "C:\\Users\\dark_knight\\anaconda3\\envs\\emotion\\lib\\site-packages\\matplotlib\\__init__.py:200: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(module.__version__) < minver:\n",
      "C:\\Users\\dark_knight\\anaconda3\\envs\\emotion\\lib\\site-packages\\setuptools\\_distutils\\version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "C:\\Users\\dark_knight\\anaconda3\\envs\\emotion\\lib\\site-packages\\matplotlib\\__init__.py:200: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(module.__version__) < minver:\n",
      "C:\\Users\\dark_knight\\anaconda3\\envs\\emotion\\lib\\site-packages\\setuptools\\_distutils\\version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "C:\\Users\\dark_knight\\anaconda3\\envs\\emotion\\lib\\site-packages\\matplotlib\\backend_bases.py:59: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(PILLOW_VERSION) >= \"3.4\":\n",
      "C:\\Users\\dark_knight\\anaconda3\\envs\\emotion\\lib\\site-packages\\setuptools\\_distutils\\version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import io\n",
    "import os\n",
    "import torch\n",
    "import warnings\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from ml_things import plot_dict, plot_confusion_matrix, fix_text\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from transformers import (set_seed,\n",
    "                          TrainingArguments,\n",
    "                          Trainer,\n",
    "                          GPT2Config,\n",
    "                          GPT2Tokenizer,\n",
    "                          AdamW, \n",
    "                          get_linear_schedule_with_warmup,\n",
    "                          GPT2ForSequenceClassification)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dc0ebeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(123)\n",
    "\n",
    "# Number of training epochs (authors on fine-tuning Bert recommend between 2 and 4).\n",
    "epochs = 4\n",
    "\n",
    "# Number of batches - depending on the max sequence length and GPU memory.\n",
    "# For 512 sequence length batch of 10 works without cuda memory issues.\n",
    "# For small sequence length can try batch of 32 or higher.\n",
    "batch_size = 32\n",
    "\n",
    "# Pad or truncate text sequences to a specific length\n",
    "# if `None` it will use maximum sequence of word piece tokens allowed by model.\n",
    "max_length = 60\n",
    "\n",
    "# Look for gpu to use. Will use `cpu` by default if no gpu found.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "model_name_or_path='skt/kogpt2-base-v2'\n",
    "\n",
    "# Dictionary of labels and their id - this will be used to convert.\n",
    "# String labels to number ids.\n",
    "labels_ids = {'sad':0,'sadness':0, 'anger':1,'angry':1,'disgust':2,'happiness':3,'fear':4,\n",
    "              'neutral':5,'surprise':6}\n",
    "\n",
    "# How many labels are we using in training.\n",
    "# This is used to decide size of classification head.\n",
    "n_labels = len(labels_ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05e175cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training4.csv\n",
      "training5.csv\n",
      "training5_2.csv\n",
      "training_korean.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "dir_path = \"./dataset\"\n",
    "data_list = []\n",
    "for (root, directories, files) in os.walk(dir_path):\n",
    "    for file in files:\n",
    "        print(file)\n",
    "        if '.csv' in file:\n",
    "            file_path = os.path.join(root, file)\n",
    "            data_list.append(pd.read_csv(file_path,encoding=\"cp949\"))\n",
    "        if '.xlsx' in file:\n",
    "            file_path = os.path.join(root, file)\n",
    "            data_list.append(pd.read_excel(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e4578eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "neu_data_list = []\n",
    "neu_data = data_list[3].iloc[:,1:]\n",
    "neu_data.columns =  [\"context\",\"label\"]\n",
    "neu_data = neu_data.replace(\"중립\",\"neutral\")\n",
    "neu_data = neu_data.replace(\"놀람\",\"surprise\")\n",
    "neu_data = neu_data.replace(\"분노\",\"angry\")\n",
    "neu_data = neu_data.replace(\"슬픔\",\"sad\")\n",
    "neu_data = neu_data.replace(\"행복\",\"happiness\")\n",
    "\n",
    "\n",
    "neu_data_list.append(neu_data[neu_data.label == \"neutral\"])\n",
    "neu_data_list.append(neu_data[neu_data.label== \"surprise\"])\n",
    "neu_data_list.append(neu_data[neu_data.label == \"angry\"])\n",
    "neu_data_list.append(neu_data[neu_data.label == \"sad\"])\n",
    "neu_data_list.append(neu_data[neu_data.label == \"happiness\"])\n",
    "\n",
    "data = data_list[0].iloc[:,1:3]\n",
    "for _ in range(1,3):\n",
    "    data = pd.concat([data,data_list[_].iloc[:,1:3]])\n",
    "data.columns = [\"context\",\"label\"]\n",
    "for buf in neu_data_list:\n",
    "    data = pd.concat([data,buf])\n",
    "data = data.replace(\"sadness\",\"sad\")\n",
    "data = data.replace(\"anger\",\"angry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6aa3307d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "neutral      47048\n",
       "sad          15972\n",
       "angry        15263\n",
       "surprise      6621\n",
       "happiness     5578\n",
       "disgust       4660\n",
       "fear          4131\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b8b63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#encoder = LabelEncoder()\n",
    "#data[\"label\"] = encoder.fit_transform(data[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d783b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, val_x, train_y, val_y = train_test_split(data[\"context\"],data[\"label\"],test_size = 0.2,stratify=data[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f04354",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.DataFrame(columns=[\"context\",\"label\"])\n",
    "train_data[\"context\"]=train_x\n",
    "train_data[\"label\"]=train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128dd2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bbd353",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025a99b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data , use_tokenizer):\n",
    "\n",
    "    # Check if path exists.\n",
    "        \"\"\"if not os.path.isdir(path):\n",
    "          # Raise error if path is invalid.\n",
    "          raise ValueError('Invalid `path` variable! Needs to be a directory')\"\"\"\n",
    "        \"\"\"self.texts = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Since the labels are defined by folders with data we loop \n",
    "        # through each label.\n",
    "        for label in ['ITscience', 'culture','economy','entertainment','health','life','politic','social','sport']:\n",
    "          sentiment_path = os.path.join(path, label)\n",
    "\n",
    "          # Get all files from path.\n",
    "          files_names = os.listdir(sentiment_path)#[:10] # Sample for debugging.\n",
    "          # Go through each file and read its content.\n",
    "            for file_name in tqdm(files_names, desc=f'{label} files'):\n",
    "                file_path = os.path.join(sentiment_path, file_name)\n",
    "\n",
    "                # Read content.\n",
    "                content = io.open(file_path, mode='r', encoding='utf-8').read()\n",
    "                # Fix any unicode issues.\n",
    "                content = fix_text(content)\n",
    "                # Save content.\n",
    "                self.texts.append(content)\n",
    "                # Save encode labels.\n",
    "                self.labels.append(label)\n",
    "        \"\"\"\n",
    "        self.texts = np.array(data[\"context\"])\n",
    "        self.labels =np.array(data[\"label\"])\n",
    "        # Number of exmaples.\n",
    "        self.n_examples = len(self.labels)\n",
    "\n",
    "\n",
    "        return\n",
    "\n",
    "    def __len__(self):\n",
    "        r\"\"\"When used `len` return the number of examples.\n",
    "\n",
    "        \"\"\"\n",
    "    \n",
    "        return self.n_examples\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        r\"\"\"Given an index return an example from the position.\n",
    "\n",
    "        Arguments:\n",
    "\n",
    "          item (:obj:`int`):\n",
    "              Index position to pick an example to return.\n",
    "\n",
    "        Returns:\n",
    "          :obj:`Dict[str, str]`: Dictionary of inputs that contain text and \n",
    "          asociated labels.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        return {'text':self.texts[item],\n",
    "                'label':self.labels[item]}\n",
    "\n",
    "\n",
    "\n",
    "class Gpt2ClassificationCollator(object):\n",
    "    r\"\"\"\n",
    "    Data Collator used for GPT2 in a classificaiton rask. \n",
    "    \n",
    "    It uses a given tokenizer and label encoder to convert any text and labels to numbers that \n",
    "    can go straight into a GPT2 model.\n",
    "\n",
    "    This class is built with reusability in mind: it can be used as is as long\n",
    "    as the `dataloader` outputs a batch in dictionary format that can be passed \n",
    "    straight into the model - `model(**batch)`.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "      use_tokenizer (:obj:`transformers.tokenization_?`):\n",
    "          Transformer type tokenizer used to process raw text into numbers.\n",
    "\n",
    "      labels_ids (:obj:`dict`):\n",
    "          Dictionary to encode any labels names into numbers. Keys map to \n",
    "          labels names and Values map to number associated to those labels.\n",
    "\n",
    "      max_sequence_len (:obj:`int`, `optional`)\n",
    "          Value to indicate the maximum desired sequence to truncate or pad text\n",
    "          sequences. If no value is passed it will used maximum sequence size\n",
    "          supported by the tokenizer and model.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, use_tokenizer, labels_encoder, max_sequence_len=None):\n",
    "\n",
    "        # Tokenizer to be used inside the class.\n",
    "        self.use_tokenizer = use_tokenizer\n",
    "        # Check max sequence length.\n",
    "        self.max_sequence_len = use_tokenizer.model_max_length if max_sequence_len is None else max_sequence_len\n",
    "        # Label encoder used inside the class.\n",
    "        self.labels_encoder = labels_encoder\n",
    "\n",
    "        return\n",
    "\n",
    "    def __call__(self, sequences):\n",
    "        r\"\"\"\n",
    "        This function allowes the class objesct to be used as a function call.\n",
    "        Sine the PyTorch DataLoader needs a collator function, I can use this \n",
    "        class as a function.\n",
    "\n",
    "        Arguments:\n",
    "\n",
    "          item (:obj:`list`):\n",
    "              List of texts and labels.\n",
    "\n",
    "        Returns:\n",
    "          :obj:`Dict[str, object]`: Dictionary of inputs that feed into the model.\n",
    "          It holddes the statement `model(**Returned Dictionary)`.\n",
    "        \"\"\"\n",
    "    \n",
    "        # Get all texts from sequences list.\n",
    "        texts = [sequence['text'] for sequence in sequences]\n",
    "        # Get all labels from sequences list.\n",
    "        labels = [sequence['label'] for sequence in sequences]\n",
    "        # Encode all labels using label encoder.\n",
    "        labels = [self.labels_encoder[label] for label in labels]\n",
    "        # Call tokenizer on all texts to convert into tensors of numbers with \n",
    "        # appropriate padding.\n",
    "        inputs = self.use_tokenizer(text=texts, return_tensors=\"pt\", padding=True, truncation=True,  max_length=self.max_sequence_len)\n",
    "        # Update the inputs with the associated encoded labels as tensor.\n",
    "        inputs.update({'labels':torch.tensor(labels)})\n",
    "\n",
    "        return inputs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77a7eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, optimizer_, scheduler_, device_):\n",
    "    \"\"\"\n",
    "      Train pytorch model on a single pass through the data loader.\n",
    "\n",
    "      It will use the global variable `model` which is the transformer model \n",
    "      loaded on `_device` that we want to train on.\n",
    "\n",
    "      This function is built with reusability in mind: it can be used as is as long\n",
    "        as the `dataloader` outputs a batch in dictionary format that can be passed \n",
    "        straight into the model - `model(**batch)`.\n",
    "\n",
    "      Arguments:\n",
    "\n",
    "          dataloader (:obj:`torch.utils.data.dataloader.DataLoader`):\n",
    "              Parsed data into batches of tensors.\n",
    "\n",
    "          optimizer_ (:obj:`transformers.optimization.AdamW`):\n",
    "              Optimizer used for training.\n",
    "\n",
    "          scheduler_ (:obj:`torch.optim.lr_scheduler.LambdaLR`):\n",
    "              PyTorch scheduler.\n",
    "\n",
    "          device_ (:obj:`torch.device`):\n",
    "              Device used to load tensors before feeding to model.\n",
    "\n",
    "      Returns:\n",
    "\n",
    "          :obj:`List[List[int], List[int], float]`: List of [True Labels, Predicted\n",
    "            Labels, Train Average Loss].\n",
    "    \"\"\"\n",
    "\n",
    "          # Use global variable for model.\n",
    "    global model\n",
    "\n",
    "          # Tracking variables.\n",
    "    predictions_labels = []\n",
    "    true_labels = []\n",
    "          # Total loss for this epoch.\n",
    "    total_loss = 0\n",
    "\n",
    "          # Put the model into training mode.\n",
    "    model.train()\n",
    "\n",
    "      # For each batch of training data...\n",
    "    for batch in tqdm(dataloader, total=len(dataloader)):\n",
    "\n",
    "            # Add original labels - use later for evaluation.\n",
    "        true_labels += batch['labels'].numpy().flatten().tolist()\n",
    "\n",
    "            # move batch to device\n",
    "        batch = {k:v.type(torch.long).to(device_) for k,v in batch.items()}\n",
    "\n",
    "            # Always clear any previously calculated gradients before performing a\n",
    "            # backward pass.\n",
    "        model.zero_grad()\n",
    "\n",
    "            # Perform a forward pass (evaluate the model on this training batch).\n",
    "            # This will return the loss (rather than the model output) because we\n",
    "            # have provided the `labels`.\n",
    "            # The documentation for this a bert model function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        outputs = model(**batch)\n",
    "\n",
    "            # The call to `model` always returns a tuple, so we need to pull the \n",
    "            # loss value out of the tuple along with the logits. We will use logits\n",
    "            # later to calculate training accuracy.\n",
    "        loss, logits = outputs[:2]\n",
    "\n",
    "            # Accumulate the training loss over all of the batches so that we can\n",
    "            # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "            # single value; the `.item()` function just returns the Python value \n",
    "            # from the tensor.\n",
    "        total_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0.\n",
    "            # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update parameters and take a step using the computed gradient.\n",
    "            # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "            # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer_.step()\n",
    "\n",
    "            # Update the learning rate.\n",
    "        scheduler_.step()\n",
    "\n",
    "            # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "\n",
    "            # Convert these logits to list of predicted labels values.\n",
    "        predictions_labels += logits.argmax(axis=-1).flatten().tolist()\n",
    "\n",
    "      # Calculate the average loss over the training data.\n",
    "    avg_epoch_loss = total_loss / len(dataloader)\n",
    "\n",
    "      # Return all true labels and prediction for future evaluations.\n",
    "    return true_labels, predictions_labels, avg_epoch_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ee43f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(dataloader, device_):\n",
    "    \"\"\"Validation function to evaluate model performance on a \n",
    "      separate set of data.\n",
    "\n",
    "      This function will return the true and predicted labels so we can use later\n",
    "      to evaluate the model's performance.\n",
    "\n",
    "      This function is built with reusability in mind: it can be used as is as long\n",
    "        as the `dataloader` outputs a batch in dictionary format that can be passed \n",
    "        straight into the model - `model(**batch)`.\n",
    "\n",
    "      Arguments:\n",
    "\n",
    "        dataloader (:obj:`torch.utils.data.dataloader.DataLoader`):\n",
    "              Parsed data into batches of tensors.\n",
    "\n",
    "        device_ (:obj:`torch.device`):\n",
    "              Device used to load tensors before feeding to model.\n",
    "\n",
    "      Returns:\n",
    "\n",
    "        :obj:`List[List[int], List[int], float]`: List of [True Labels, Predicted\n",
    "            Labels, Train Average Loss]\n",
    "    \"\"\"\n",
    "\n",
    "  # Use global variable for model.\n",
    "    global model\n",
    "\n",
    "  # Tracking variables\n",
    "    predictions_labels = []\n",
    "    true_labels = []\n",
    "      #total loss for this epoch.\n",
    "    total_loss = 0\n",
    "\n",
    "  # Put the model in evaluation mode--the dropout layers behave differently\n",
    "  # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "  # Evaluate data for one epoch\n",
    "    for batch in tqdm(dataloader, total=len(dataloader)):\n",
    "\n",
    "        # add original labels\n",
    "        true_labels += batch['labels'].numpy().flatten().tolist()\n",
    "\n",
    "        # move batch to device\n",
    "        batch = {k:v.type(torch.long).to(device_) for k,v in batch.items()}\n",
    "\n",
    "        # Telling the model not to compute or store gradients, saving memory and\n",
    "        # speeding up validation\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # This will return the logits rather than the loss because we have\n",
    "            # not provided labels.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            outputs = model(**batch)\n",
    "\n",
    "            # The call to `model` always returns a tuple, so we need to pull the \n",
    "            # loss value out of the tuple along with the logits. We will use logits\n",
    "            # later to to calculate training accuracy.\n",
    "            loss, logits = outputs[:2]\n",
    "\n",
    "            # Move logits and labels to CPU\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "\n",
    "            # Accumulate the training loss over all of the batches so that we can\n",
    "            # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "            # single value; the `.item()` function just returns the Python value \n",
    "            # from the tensor.\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # get predicitons to list\n",
    "            predict_content = logits.argmax(axis=-1).flatten().tolist()\n",
    "\n",
    "            # update list\n",
    "            predictions_labels += predict_content\n",
    "\n",
    "  # Calculate the average loss over the training data.\n",
    "    avg_epoch_loss = total_loss / len(dataloader)\n",
    "\n",
    "  # Return all true labels and prediciton for future evaluations.\n",
    "    return true_labels, predictions_labels, avg_epoch_loss\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8a7280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model configuration.\n",
    "print('Loading configuraiton...')\n",
    "model_config = GPT2Config.from_pretrained(pretrained_model_name_or_path=model_name_or_path, num_labels=n_labels)\n",
    "\n",
    "# Get model's tokenizer.\n",
    "print('Loading tokenizer...')\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n",
    "  bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n",
    "  pad_token='<pad>', mask_token='<mask>') \n",
    "# default to left padding\n",
    "tokenizer.padding_side = \"left\"\n",
    "# Define PAD Token = EOS Token = 50256\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "# Get the actual model.\n",
    "print('Loading model...')\n",
    "model = GPT2ForSequenceClassification.from_pretrained(pretrained_model_name_or_path=model_name_or_path, config=model_config)\n",
    "\n",
    "# resize model embedding to match new tokenizer\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# fix model padding token id\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# Load model to defined device.\n",
    "model.to(device)\n",
    "print('Model loaded to `%s`'%device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb06d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data collator to encode text and labels into numbers.\n",
    "gpt2_classificaiton_collator = Gpt2ClassificationCollator(use_tokenizer=tokenizer, \n",
    "                                                          labels_encoder=labels_ids, \n",
    "                                                          max_sequence_len=max_length)\n",
    "\n",
    "\n",
    "print('Dealing with Train...')\n",
    "# Create pytorch dataset.\n",
    "train_dataset = NewsDataset(data =data, \n",
    "                               use_tokenizer=tokenizer)\n",
    "print('Created `train_dataset` with %d examples!'%len(train_dataset))\n",
    "\n",
    "# Move pytorch dataset into dataloader.\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=gpt2_classificaiton_collator)\n",
    "print('Created `train_dataloader` with %d batches!'%len(train_dataloader))\n",
    "\n",
    "print()\n",
    "\n",
    "#print('Dealing with Validation...')\n",
    "# Create pytorch dataset.\n",
    "#valid_dataset = NewsDataset(path='/content/drive/MyDrive/newsTopicClassification/sampling_test_data', \n",
    "#                               use_tokenizer=tokenizer)\n",
    "#print('Created `valid_dataset` with %d examples!'%len(valid_dataset))\n",
    "\n",
    "# Move pytorch dataset into dataloader.\n",
    "#valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=gpt2_classificaiton_collator)\n",
    "#print('Created `eval_dataloader` with %d batches!'%len(valid_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4d3f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "# I believe the 'W' stands for 'Weight Decay fix\"\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # default is 1e-8.\n",
    "                  )\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "# `train_dataloader` contains batched data so `len(train_dataloader)` gives \n",
    "# us the number of batches.\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)\n",
    "\n",
    "# Store the average loss after each epoch so we can plot them.\n",
    "all_loss = {'train_loss':[], 'val_loss':[]}\n",
    "all_acc = {'train_acc':[], 'val_acc':[]}\n",
    "\n",
    "# Loop through each epoch.\n",
    "print('Epoch')\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print()\n",
    "    print('Training on batches...')\n",
    "    # Perform one full pass over the training set.\n",
    "    train_labels, train_predict, train_loss = train(train_dataloader, optimizer, scheduler, device)\n",
    "    train_acc = accuracy_score(train_labels, train_predict)\n",
    "\n",
    "  # Get prediction form model on validation data. \n",
    "    print('Validation on batches...')\n",
    "    #valid_labels, valid_predict, val_loss = validation(valid_dataloader, device)\n",
    "    #val_acc = accuracy_score(valid_labels, valid_predict)\n",
    "\n",
    "  # Print loss and accuracy values to see how training evolves.\n",
    "    #print(\"  train_loss: %.5f - val_loss: %.5f - train_acc: %.5f - valid_acc: %.5f\"%(train_loss, val_loss, train_acc, val_acc))\n",
    "    print(\"train_loss: {0:.5f} - train_acc: {1:.5f}\".format(train_loss, train_acc))\n",
    "    print()\n",
    "\n",
    "  # Store the loss value for plotting the learning curve.\n",
    "    all_loss['train_loss'].append(train_loss)\n",
    "    #all_loss['val_loss'].append(val_loss)\n",
    "    all_acc['train_acc'].append(train_acc)\n",
    "    #all_acc['val_acc'].append(val_acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0363cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss curves.\n",
    "plot_dict(all_loss, use_xlabel='Epochs', use_ylabel='Value', use_linestyles=['-', '--'])\n",
    "\n",
    "# Plot accuracy curves.\n",
    "plot_dict(all_acc, use_xlabel='Epochs', use_ylabel='Value', use_linestyles=['-', '--'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401282ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a1339a",
   "metadata": {},
   "outputs": [],
   "source": [
    " torch.cuda.is_available() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a565182e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.get_device_name(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1971086a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0179679",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Device:', device)\n",
    "print('가능',torch.cuda.device_count())\n",
    "print('Current cuda device:', torch.cuda.current_device())\n",
    "print('Count of using GPUs:', torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e70fb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.rand(5,4,device = 'cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c46e8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kogpt2",
   "language": "python",
   "name": "emotion"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
