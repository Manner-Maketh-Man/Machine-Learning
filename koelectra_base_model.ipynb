{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import Trainer, TrainingArguments, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Load Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparing_data_1(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    # SettingWithCopyWarning\n",
    "    data = df.copy()\n",
    "\n",
    "    def func(c):\n",
    "        if c == \"sad\" : return \"sadness\"\n",
    "        elif c == \"anger\" : return \"angry\"\n",
    "        else : return c\n",
    "        \n",
    "    data = data[[\"발화문\", \"상황\"]]\n",
    "    data.columns = [\"sentence\", \"class\"]\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    data[\"class\"] = data[\"class\"].apply(func)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparing_data_2(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    # SettingWithCopyWarning\n",
    "    data = df.copy()\n",
    "    \n",
    "    def func(c):\n",
    "        if c == \"분노\" : return \"angry\"\n",
    "        elif c == \"혐오\" : return \"disgust\"\n",
    "        elif c == \"중립\" : return \"neutral\"\n",
    "        elif c == \"놀람\" : return \"surprise\"\n",
    "        elif c == \"행복\" : return \"happiness\"\n",
    "        elif c == \"공포\" : return \"fear\"\n",
    "        elif c == \"슬픔\" : return \"sadness\"\n",
    "        else : return np.nan\n",
    "\n",
    "    # cleaning data    \n",
    "    data = data[[\"Unnamed: 1\" ,\"Unnamed: 2\"]]\n",
    "    data.columns = [\"sentence\", \"class\"]\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    data[\"class\"] = data[\"class\"].apply(func)\n",
    "    data.dropna(inplace=True)\n",
    "\n",
    "    # imbalanced data \n",
    "    \"\"\"\n",
    "    \n",
    "    # all data size\n",
    "    data.shape >= 50000\n",
    "\n",
    "    # one column data size\n",
    "    data[\"neutral\"].shape >= 40000\n",
    "    \n",
    "    # apply random undersampling\n",
    "    data[\"neutral\"].shape : 10000\n",
    "\n",
    "    \"\"\"\n",
    "    neutral_index = list(data[data[\"class\"] == \"neutral\"].index)\n",
    "    remove_index = random.sample(neutral_index,33786)\n",
    "    data.drop(remove_index, inplace=True)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "dataset_1_1 = \"./data/year_4.csv\"\n",
    "dataset_1_2 = \"./data/year_5_1.csv\"\n",
    "dataset_1_3 = \"./data/year_5_2.csv\"\n",
    "\n",
    "train1 = preparing_data_1(pd.read_csv(dataset_1_1, index_col=0, encoding=\"cp949\"))\n",
    "train2 = preparing_data_1(pd.read_csv(dataset_1_2, index_col=0, encoding=\"cp949\"))\n",
    "train3 = preparing_data_1(pd.read_csv(dataset_1_3, index_col=0, encoding=\"cp949\"))\n",
    "\n",
    "#\n",
    "dataset_2_0 = \"./한국어_연속적_대화_데이터셋.xlsx\"\n",
    "\n",
    "train4 = preparing_data_2(pd.read_excel(dataset_2_0, index_col=0))\n",
    "\n",
    "# concat\n",
    "train = pd.DataFrame()\n",
    "train = pd.concat([train, train1], axis=0, ignore_index=True)\n",
    "train = pd.concat([train, train2], axis=0, ignore_index=True)\n",
    "train = pd.concat([train, train3], axis=0, ignore_index=True)\n",
    "train = pd.concat([train, train4], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'angry': 15263,\n",
       "         'sadness': 15972,\n",
       "         'fear': 4229,\n",
       "         'disgust': 4880,\n",
       "         'neutral': 13262,\n",
       "         'happiness': 5578,\n",
       "         'surprise': 6621})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(train['class'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>어, 청소 니가 대신 해 줘!</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>둘 다 청소 하기 싫어. 귀찮아.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>둘 다 하기 싫어서 화내.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>그럼 방세는 어떡해.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>권태긴줄 알았는데 다른 사람이 생겼나보더라고.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    sentence  class\n",
       "0           어, 청소 니가 대신 해 줘!      5\n",
       "1         둘 다 청소 하기 싫어. 귀찮아.      5\n",
       "2             둘 다 하기 싫어서 화내.      5\n",
       "3                그럼 방세는 어떡해.      5\n",
       "4  권태긴줄 알았는데 다른 사람이 생겼나보더라고.      0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label encoding\n",
    "d = {\"sadness\":0,\n",
    "     \"fear\":1,\n",
    "     \"disgust\":2,\n",
    "     \"neutral\":3,\n",
    "     \"happiness\":4,\n",
    "     \"angry\":5,\n",
    "     \"surprise\":6}\n",
    "\n",
    "train['class'] = train['class'].map(d)\n",
    "train.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) split train / test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(train, test_size=0.2, shuffle=True, stratify=train['class'], random_state = 1234)\n",
    "train_label = train['class'].values\n",
    "test_label = test['class'].values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4) tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER_NAME = \"monologg/koelectra-base-v3-discriminator\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 64\n",
    "\n",
    "tokenized_train_sentences = tokenizer(\n",
    "    list(train[\"sentence\"]),\n",
    "    return_tensors=\"pt\",        # pytorch return form\n",
    "    max_length = MAX_LEN,       # set max token length\n",
    "    padding = True,             # set zeropadding\n",
    "    truncation = True,          # set truncate\n",
    "    add_special_tokens = True   # \n",
    ")\n",
    "\n",
    "tokenized_test_sentences = tokenizer(\n",
    "    list(test[\"sentence\"]),\n",
    "    return_tensors=\"pt\",        # pytorch return form\n",
    "    max_length = MAX_LEN,       # set max token length\n",
    "    padding = True,             # set zeropadding\n",
    "    truncation = True,          # set truncate\n",
    "    add_special_tokens = True   # \n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5) make pyTorch form dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MakeDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MakeDataset(tokenized_train_sentences, train_label)\n",
    "test_dataset = MakeDataset(tokenized_test_sentences, test_label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Model Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/koelectra-base-v3-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ElectraForSequenceClassification(\n",
       "  (electra): ElectraModel(\n",
       "    (embeddings): ElectraEmbeddings(\n",
       "      (word_embeddings): Embedding(35000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): ElectraEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): ElectraClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=7, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_NAME = \"monologg/koelectra-base-v3-discriminator\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=7)\n",
    "model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./',\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=64,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=500,\n",
    "    save_total_limit=2\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4가지 평가지표 사용 \n",
    "# 정확도(accuracy), 정밀도(precision), 재현율(recall), F1 Score\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy' : acc,\n",
    "        'f1' : f1,\n",
    "        'precision' : precision,\n",
    "        'recall' : recall\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4) training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(eval_dataset=test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
